{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "# Training Analysis\n",
    "\n",
    "Post-training analysis of the VAE + LCFM pipeline.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [VAE Results](#vae-results)\n",
    "   - 2.1. [Loss Curves](#vae-loss)\n",
    "   - 2.2. [Reconstruction & Residuals](#vae-recon)\n",
    "   - 2.3. [Random Samples](#vae-random)\n",
    "3. [LCFM Results](#lcfm-results)\n",
    "   - 3.1. [Loss Curves](#lcfm-loss)\n",
    "   - 3.2. [Sample Quality Progression](#lcfm-prog)\n",
    "   - 3.3. [Conditional Generation](#lcfm-gen)\n",
    "   - 3.4. [Random Generation](#lcfm-random)\n",
    "   - 3.5. [Flow Trajectory](#lcfm-traj)\n",
    "4. [Summary](#summary)\n",
    "   - [Latent Space Visualization](#latent-pca)\n",
    "   - [Per-Band Pixel Distributions](#pixel-dist)\n",
    "   - [Summary Statistics](#summary-stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-setup",
   "metadata": {},
   "source": [
    "## 1. Setup <a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from galgenai.models import VAE, VAEEncoder, LCFM\n",
    "from galgenai.data.hsc import get_dataset_and_loaders\n",
    "from galgenai import get_device\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "def strip_compile_prefix(state_dict):\n",
    "    \"\"\"Strip '_orig_mod.' prefix added by torch.compile().\"\"\"\n",
    "    prefix = \"_orig_mod.\"\n",
    "    return {k.removeprefix(prefix): v for k, v in state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- USER-EDITABLE PARAMETERS ----\n",
    "\n",
    "# Paths\n",
    "data_path = \"../data/hsc_mmu_mini/\"\n",
    "output_dir = Path(\"../pipeline_output\")\n",
    "\n",
    "# Model hyperparameters (must match training)\n",
    "in_channels = 5\n",
    "latent_dim = 32\n",
    "input_size = 64\n",
    "lcfm_base_channels = 64\n",
    "lcfm_beta = 0.001\n",
    "\n",
    "# Visualization parameters\n",
    "n_plot = 4\n",
    "n_gen_lcfm = 7\n",
    "num_ode_steps = 50\n",
    "\n",
    "# Band names and colormap\n",
    "bands = [f\"hsc-{b}\" for b in \"grizy\"]\n",
    "n_bands = len(bands)\n",
    "cmap = plt.get_cmap(\"magma\")\n",
    "cmap.set_bad(\"0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dataset_raw = load_from_disk(data_path)\n",
    "dataset, train_loader, val_loader = get_dataset_and_loaders(\n",
    "    dataset_raw,\n",
    "    nx=input_size,\n",
    "    batch_size=128,\n",
    "    num_workers=0,\n",
    "    split=0.8,\n",
    ")\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-05-load-vae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VAE from best checkpoint\n",
    "vae_ckpt_path = output_dir / \"vae\" / \"checkpoints\" / \"best.pt\"\n",
    "vae_ckpt = torch.load(vae_ckpt_path, map_location=device, weights_only=False)\n",
    "\n",
    "vae = VAE(\n",
    "    in_channels=in_channels,\n",
    "    latent_dim=latent_dim,\n",
    "    input_size=input_size,\n",
    ")\n",
    "vae.load_state_dict(strip_compile_prefix(vae_ckpt[\"model_state_dict\"]))\n",
    "vae.to(device).eval()\n",
    "\n",
    "vae_loss_history = vae_ckpt[\"loss_history\"]\n",
    "\n",
    "n_vae_params = sum(p.numel() for p in vae.parameters())\n",
    "print(f\"VAE parameters: {n_vae_params:,}\")\n",
    "print(f\"VAE training steps: {vae_ckpt['global_step']}\")\n",
    "print(f\"VAE best loss: {vae_ckpt['best_loss']:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06-load-lcfm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LCFM from best checkpoint\n",
    "encoder = VAEEncoder(\n",
    "    in_channels=in_channels,\n",
    "    latent_dim=latent_dim,\n",
    "    input_size=input_size,\n",
    ")\n",
    "encoder_path = output_dir / \"encoder.pt\"\n",
    "encoder.load_state_dict(torch.load(encoder_path, map_location=device))\n",
    "\n",
    "lcfm_ckpt_path = output_dir / \"lcfm\" / \"checkpoints\" / \"best.pt\"\n",
    "lcfm_ckpt = torch.load(lcfm_ckpt_path, map_location=device, weights_only=False)\n",
    "\n",
    "lcfm = LCFM(\n",
    "    vae_encoder=encoder,\n",
    "    latent_dim=latent_dim,\n",
    "    in_channels=in_channels,\n",
    "    input_size=input_size,\n",
    "    base_channels=lcfm_base_channels,\n",
    "    beta=lcfm_beta,\n",
    ")\n",
    "lcfm.load_state_dict(strip_compile_prefix(lcfm_ckpt[\"model_state_dict\"]))\n",
    "lcfm.to(device).eval()\n",
    "\n",
    "lcfm_loss_history = lcfm_ckpt[\"loss_history\"]\n",
    "\n",
    "n_lcfm_total = sum(p.numel() for p in lcfm.parameters())\n",
    "n_lcfm_trainable = sum(p.numel() for p in lcfm.parameters() if p.requires_grad)\n",
    "print(f\"LCFM total parameters: {n_lcfm_total:,}\")\n",
    "print(f\"LCFM trainable parameters: {n_lcfm_trainable:,}\")\n",
    "print(f\"LCFM training steps: {lcfm_ckpt['global_step']}\")\n",
    "print(f\"LCFM best loss: {lcfm_ckpt['best_loss']:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-vae",
   "metadata": {},
   "source": [
    "## 2. VAE Results <a id=\"vae-results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-vae-loss",
   "metadata": {},
   "source": [
    "### 2.1. Loss Curves <a id=\"vae-loss\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-08-vae-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE loss curves (epoch-based)\n",
    "vae_epochs = [e[\"epoch\"] for e in vae_loss_history]\n",
    "\n",
    "vae_total = [e[\"total_loss\"] for e in vae_loss_history]\n",
    "vae_recon = [e[\"recon_loss\"] for e in vae_loss_history]\n",
    "vae_kl = [e[\"kl_loss\"] for e in vae_loss_history]\n",
    "\n",
    "vae_val_total = [\n",
    "    e[\"val_total_loss\"] for e in vae_loss_history if \"val_total_loss\" in e\n",
    "]\n",
    "vae_val_recon = [\n",
    "    e[\"val_recon_loss\"] for e in vae_loss_history if \"val_recon_loss\" in e\n",
    "]\n",
    "vae_val_kl = [e[\"val_kl_loss\"] for e in vae_loss_history if \"val_kl_loss\" in e]\n",
    "vae_val_epochs = [\n",
    "    e[\"epoch\"] for e in vae_loss_history if \"val_total_loss\" in e\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3.5))\n",
    "\n",
    "for ax, train, val, val_ep, title in zip(\n",
    "    axs,\n",
    "    [vae_total, vae_recon, vae_kl],\n",
    "    [vae_val_total, vae_val_recon, vae_val_kl],\n",
    "    [vae_val_epochs] * 3,\n",
    "    [\"Total Loss\", \"Recon Loss\", \"KL Divergence\"],\n",
    "    strict=True,\n",
    "):\n",
    "    ax.plot(vae_epochs, train, label=\"Train\")\n",
    "    if val:\n",
    "        ax.semilogy(val_ep, val, label=\"Val\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "fig.suptitle(\"VAE Training Loss\", fontsize=14)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-vae-recon",
   "metadata": {},
   "source": [
    "### 2.2. Reconstruction & Residuals <a id=\"vae-recon\"></a>\n",
    "\n",
    "Compare with [3.3. LCFM Conditional Generation](#lcfm-gen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11-vae-recon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE reconstruction grid\n",
    "plt.close(\"all\")\n",
    "\n",
    "imgs = torch.stack(next(iter(val_loader)))  # [3, batch, 5, 64, 64]\n",
    "imgs = imgs[:, :n_plot, :, :, :]\n",
    "with torch.no_grad():\n",
    "    recs, _, _ = vae(imgs[0, :, :, :, :].to(device))\n",
    "\n",
    "fig, axs = plt.subplots(2 * n_plot, n_bands, figsize=(8, 10))\n",
    "for i in range(n_plot):\n",
    "    img, ivr, msk = imgs[:, i, :, :, :].cpu()\n",
    "    rec = recs[i].cpu()\n",
    "    for j, band in enumerate(bands):\n",
    "        vmin, vmax = img[j].min(), img[j].max()\n",
    "        axs[2 * i, j].imshow(\n",
    "            img[j] / msk[j],\n",
    "            origin=\"lower\",\n",
    "            cmap=cmap,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        axs[2 * i + 1, j].imshow(\n",
    "            rec[j] / msk[j],\n",
    "            origin=\"lower\",\n",
    "            cmap=cmap,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        if i == 0:\n",
    "            axs[0, j].set_title(band)\n",
    "    axs[2 * i, 0].set_ylabel(\"Data\")\n",
    "    axs[2 * i + 1, 0].set_ylabel(\"Reconst.\")\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.xaxis.set_ticks([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "fig.suptitle(\"Reconstruction vs truth (validation set)\")\n",
    "fig.subplots_adjust(left=0.05, right=0.95, bottom=0.02, top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13-residuals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual maps: (data - reconstruction) * mask\n",
    "cmap_resid = plt.get_cmap(\"RdBu_r\")\n",
    "cmap_resid.set_bad(\"0.5\")\n",
    "\n",
    "fig, axs = plt.subplots(n_plot, n_bands, figsize=(8, 6))\n",
    "for i in range(n_plot):\n",
    "    img, ivr, msk = imgs[:, i, :, :, :].cpu()\n",
    "    rec = recs[i].cpu()\n",
    "    for j, band in enumerate(bands):\n",
    "        residual = (img[j] - rec[j]) * msk[j]\n",
    "        abs_max = residual.abs().max().item()\n",
    "        axs[i, j].imshow(\n",
    "            residual,\n",
    "            origin=\"lower\",\n",
    "            cmap=cmap_resid,\n",
    "            vmin=-abs_max,\n",
    "            vmax=abs_max,\n",
    "        )\n",
    "        if i == 0:\n",
    "            axs[0, j].set_title(band)\n",
    "    axs[i, 0].set_ylabel(f\"Galaxy #{i + 1}\")\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.xaxis.set_ticks([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "fig.suptitle(\"Residuals: (Data - Reconstruction) * Mask\")\n",
    "fig.subplots_adjust(left=0.05, right=0.95, bottom=0.02, top=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-vae-random",
   "metadata": {},
   "source": [
    "### 2.3. Random Samples <a id=\"vae-random\"></a>\n",
    "\n",
    "Compare with [3.4. LCFM Random Generation](#lcfm-random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15-vae-random",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE random generation from N(0, I) prior\n",
    "fig, axs = plt.subplots(n_plot, n_bands, figsize=(8, 6))\n",
    "with torch.no_grad():\n",
    "    gen_imgs = vae.generate(n_plot, device)\n",
    "for i in range(n_plot):\n",
    "    img = gen_imgs[i].cpu()\n",
    "    for j, band in enumerate(bands):\n",
    "        axs[i, j].imshow(img[j], origin=\"lower\", cmap=cmap)\n",
    "        if i == 0:\n",
    "            axs[0, j].set_title(band)\n",
    "    axs[i, 0].set_ylabel(f\"Galaxy #{i + 1}\")\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.xaxis.set_ticks([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "fig.suptitle(\"Randomly generated images\")\n",
    "fig.subplots_adjust(left=0.05, right=0.95, bottom=0.02, top=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-lcfm",
   "metadata": {},
   "source": [
    "## 3. LCFM Results <a id=\"lcfm-results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-lcfm-loss",
   "metadata": {},
   "source": [
    "### 3.1. Loss Curves <a id=\"lcfm-loss\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09-lcfm-loss",
   "metadata": {},
   "outputs": [],
   "source": "# LCFM loss curves (step-based, train + val)\nlcfm_steps = [e[\"step\"] for e in lcfm_loss_history]\nlcfm_total = [e[\"total_loss\"] for e in lcfm_loss_history]\nlcfm_flow = [e[\"flow_loss\"] for e in lcfm_loss_history]\nlcfm_kl = [e[\"kl_loss\"] for e in lcfm_loss_history]\n\nlcfm_val_total = [\n    e[\"val_total_loss\"] for e in lcfm_loss_history if \"val_total_loss\" in e\n]\nlcfm_val_flow = [\n    e[\"val_flow_loss\"] for e in lcfm_loss_history if \"val_flow_loss\" in e\n]\nlcfm_val_kl = [\n    e[\"val_kl_loss\"] for e in lcfm_loss_history if \"val_kl_loss\" in e\n]\nlcfm_val_steps = [\n    e[\"step\"] for e in lcfm_loss_history if \"val_total_loss\" in e\n]\n\nfig, axs = plt.subplots(1, 3, figsize=(12, 3.5))\n\nfor ax, train, val, title in zip(\n    axs,\n    [lcfm_total, lcfm_flow, lcfm_kl],\n    [lcfm_val_total, lcfm_val_flow, lcfm_val_kl],\n    [\"Total Loss\", \"Flow Loss\", \"KL Divergence\"],\n    strict=True,\n):\n    ax.semilogy(lcfm_steps, train, label=\"Train\")\n    if val:\n        ax.semilogy(lcfm_val_steps, val, label=\"Val\")\n    ax.set_xlabel(\"Step\")\n    ax.set_title(title)\n    ax.legend()\n\nfig.suptitle(\"LCFM Training Loss\", fontsize=14)\nfig.tight_layout()"
  },
  {
   "cell_type": "markdown",
   "id": "h-lcfm-prog",
   "metadata": {},
   "source": [
    "### 3.2. Sample Quality Progression <a id=\"lcfm-prog\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19-lcfm-prog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-saved LCFM samples at different training steps\n",
    "sample_dir = output_dir / \"lcfm\" / \"samples\"\n",
    "sample_files = sorted(sample_dir.glob(\"samples_step_*.pt\"))\n",
    "print(f\"Found {len(sample_files)} sample files\")\n",
    "\n",
    "n_steps_show = len(sample_files)\n",
    "# Show first sample from each step\n",
    "i_show = 0\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    n_steps_show,\n",
    "    2 * n_bands,\n",
    "    figsize=(14, 2 * n_steps_show),\n",
    ")\n",
    "if n_steps_show == 1:\n",
    "    axs = axs[None, :]\n",
    "\n",
    "for row, sf in enumerate(sample_files):\n",
    "    data = torch.load(sf, map_location=\"cpu\", weights_only=False)\n",
    "    samp = data[\"samples\"][i_show]  # [5, 64, 64]\n",
    "    cond = data[\"conditioning\"][i_show]  # [5, 64, 64]\n",
    "\n",
    "    step_num = sf.stem.split(\"_\")[-1]\n",
    "\n",
    "    for j in range(n_bands):\n",
    "        vmin, vmax = cond[j].min(), cond[j].max()\n",
    "        axs[row, j].imshow(\n",
    "            cond[j],\n",
    "            origin=\"lower\",\n",
    "            cmap=cmap,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        axs[row, n_bands + j].imshow(\n",
    "            samp[j],\n",
    "            origin=\"lower\",\n",
    "            cmap=cmap,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        if row == 0:\n",
    "            axs[0, j].set_title(bands[j])\n",
    "            axs[0, n_bands + j].set_title(bands[j])\n",
    "\n",
    "    axs[row, 0].set_ylabel(f\"Step {step_num}\")\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.xaxis.set_ticks([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "\n",
    "fig.text(0.27, 0.99, \"Conditioning\", ha=\"center\", va=\"top\", fontsize=13)\n",
    "fig.text(0.73, 0.99, \"Generated\", ha=\"center\", va=\"top\", fontsize=13)\n",
    "fig.suptitle(\"LCFM Sample Quality Progression\", fontsize=14, y=1.02)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-lcfm-gen",
   "metadata": {},
   "source": [
    "### 3.3. Conditional Generation <a id=\"lcfm-gen\"></a>\n",
    "\n",
    "Compare with [2.2. VAE Reconstruction & Residuals](#vae-recon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17-lcfm-gen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCFM conditional samples\n",
    "con_img = torch.stack(next(iter(val_loader)))[0, 0, :, :, :]\n",
    "con_imgs = torch.stack([con_img for _ in range(n_gen_lcfm)]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    lcfm_gen = lcfm.sample(con_imgs, num_steps=num_ode_steps)\n",
    "\n",
    "fig, axs = plt.subplots(1 + n_gen_lcfm, n_bands, figsize=(8, 10))\n",
    "for i in range(1 + n_gen_lcfm):\n",
    "    img = con_img.cpu() if i == 0 else lcfm_gen[i - 1].cpu()\n",
    "    for j, band in enumerate(bands):\n",
    "        vmin, vmax = con_img[j].cpu().min(), con_img[j].cpu().max()\n",
    "        axs[i, j].imshow(\n",
    "            img[j],\n",
    "            origin=\"lower\",\n",
    "            cmap=cmap,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        if i == 0:\n",
    "            axs[0, j].set_title(band)\n",
    "    if i == 0:\n",
    "        axs[i, 0].set_ylabel(\"Original\")\n",
    "    else:\n",
    "        axs[i, 0].set_ylabel(\"Generated\")\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.xaxis.set_ticks([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "fig.suptitle(\"Generated galaxies vs their conditional\")\n",
    "fig.subplots_adjust(left=0.05, right=0.95, bottom=0.02, top=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-lcfm-random",
   "metadata": {},
   "source": [
    "### 3.4. Random Generation <a id=\"lcfm-random\"></a>\n",
    "\n",
    "Compare with [2.3. VAE Random Samples](#vae-random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5y6ufbjihys",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCFM unconditional generation: sample latents from N(0, I)\n",
    "# instead of encoding a real image\n",
    "with torch.no_grad():\n",
    "    f = torch.randn(n_plot, latent_dim, device=device)\n",
    "    x = torch.randn(\n",
    "        n_plot,\n",
    "        in_channels,\n",
    "        input_size,\n",
    "        input_size,\n",
    "        device=device,\n",
    "    )\n",
    "    dt = 1.0 / num_ode_steps\n",
    "    for step in range(num_ode_steps):\n",
    "        t = torch.full((n_plot,), step * dt, device=device)\n",
    "        v = lcfm.velocity_net(x, f, t)\n",
    "        x = x + v * dt\n",
    "    lcfm_random = x\n",
    "\n",
    "fig, axs = plt.subplots(n_plot, n_bands, figsize=(8, 6))\n",
    "for i in range(n_plot):\n",
    "    img = lcfm_random[i].cpu()\n",
    "    for j, band in enumerate(bands):\n",
    "        axs[i, j].imshow(img[j], origin=\"lower\", cmap=cmap)\n",
    "        if i == 0:\n",
    "            axs[0, j].set_title(band)\n",
    "    axs[i, 0].set_ylabel(f\"Galaxy #{i + 1}\")\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.xaxis.set_ticks([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "fig.suptitle(\"LCFM unconditional generation (prior latents)\")\n",
    "fig.subplots_adjust(left=0.05, right=0.95, bottom=0.02, top=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-lcfm-traj",
   "metadata": {},
   "source": [
    "### 3.5. Flow Trajectory <a id=\"lcfm-traj\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21-traj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODE trajectory visualization (single band: i-band, index 2)\n",
    "cond_img = torch.stack(next(iter(val_loader)))[0, 0:1, :, :, :].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    final, trajectory = lcfm.sample(\n",
    "        cond_img,\n",
    "        num_steps=num_ode_steps,\n",
    "        return_trajectory=True,\n",
    "    )\n",
    "\n",
    "# Pick ~8 evenly spaced time steps from trajectory\n",
    "n_traj_show = 8\n",
    "indices = np.linspace(0, len(trajectory) - 1, n_traj_show, dtype=int)\n",
    "band_idx = 2  # i-band\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    1,\n",
    "    1 + n_traj_show,\n",
    "    figsize=(2 * (1 + n_traj_show), 2),\n",
    ")\n",
    "\n",
    "# Conditioning image\n",
    "axs[0].imshow(\n",
    "    cond_img[0, band_idx].cpu(),\n",
    "    origin=\"lower\",\n",
    "    cmap=cmap,\n",
    ")\n",
    "axs[0].set_title(\"Cond.\")\n",
    "\n",
    "# Trajectory snapshots\n",
    "for k, idx in enumerate(indices):\n",
    "    t_val = idx / (len(trajectory) - 1)\n",
    "    axs[k + 1].imshow(\n",
    "        trajectory[idx][0, band_idx].cpu(),\n",
    "        origin=\"lower\",\n",
    "        cmap=cmap,\n",
    "    )\n",
    "    axs[k + 1].set_title(f\"t={t_val:.2f}\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.xaxis.set_ticks([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "fig.suptitle(f\"Flow Trajectory ({bands[band_idx]})\", fontsize=13)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-summary",
   "metadata": {},
   "source": [
    "## 4. Summary <a id=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-latent-pca",
   "metadata": {},
   "source": [
    "### Latent Space Visualization <a id=\"latent-pca\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23-pca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of VAE latent space\n",
    "all_mu = []\n",
    "all_flux = []\n",
    "n_collect = 500\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        data = torch.stack(batch)[0].to(device)\n",
    "        mu, _ = vae.encoder(data)\n",
    "        total_flux = data.sum(dim=(1, 2, 3))\n",
    "        all_mu.append(mu.cpu())\n",
    "        all_flux.append(total_flux.cpu())\n",
    "        if sum(m.shape[0] for m in all_mu) >= n_collect:\n",
    "            break\n",
    "\n",
    "all_mu = torch.cat(all_mu)[:n_collect]\n",
    "all_flux = torch.cat(all_flux)[:n_collect]\n",
    "\n",
    "# PCA via torch.pca_lowrank\n",
    "mu_centered = all_mu - all_mu.mean(dim=0)\n",
    "U, S, V = torch.pca_lowrank(mu_centered, q=2)\n",
    "projected = mu_centered @ V  # [n_collect, 2]\n",
    "\n",
    "# Explained variance ratio\n",
    "total_var = (S**2).sum()\n",
    "explained = (S**2) / total_var\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sc = ax.scatter(\n",
    "    projected[:, 0].numpy(),\n",
    "    projected[:, 1].numpy(),\n",
    "    c=torch.log10(all_flux).numpy(),\n",
    "    s=8,\n",
    "    alpha=0.7,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "fig.colorbar(sc, ax=ax, label=\"log10(total flux)\")\n",
    "ax.set_xlabel(f\"PC1 ({explained[0]:.1%} var)\")\n",
    "ax.set_ylabel(f\"PC2 ({explained[1]:.1%} var)\")\n",
    "ax.set_title(\"VAE Latent Space (PCA)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-pixel-dist",
   "metadata": {},
   "source": [
    "### Per-Band Pixel Distributions <a id=\"pixel-dist\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25-pixel-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixel value histograms: real vs VAE recon vs VAE random vs LCFM\n",
    "# Collect a batch of real, reconstructed, random, and LCFM images\n",
    "val_batch = torch.stack(next(iter(val_loader)))\n",
    "real_data = val_batch[0].cpu()  # [batch, 5, 64, 64]\n",
    "\n",
    "with torch.no_grad():\n",
    "    vae_recs, _, _ = vae(real_data.to(device))\n",
    "    vae_recs = vae_recs.cpu()\n",
    "    vae_rand = vae.generate(real_data.shape[0], device).cpu()\n",
    "\n",
    "    lcfm_cond = real_data[:n_gen_lcfm].to(device)\n",
    "    lcfm_samp = lcfm.sample(lcfm_cond, num_steps=num_ode_steps).cpu()\n",
    "\n",
    "fig, axs = plt.subplots(1, n_bands, figsize=(14, 3))\n",
    "for j, band in enumerate(bands):\n",
    "    real_pix = real_data[:, j].flatten().numpy()\n",
    "    p99 = np.percentile(real_pix, 99)\n",
    "    bin_edges = np.linspace(0, p99, 80)\n",
    "\n",
    "    axs[j].hist(\n",
    "        real_pix,\n",
    "        bins=bin_edges,\n",
    "        density=True,\n",
    "        alpha=0.5,\n",
    "        label=\"Real\",\n",
    "    )\n",
    "    axs[j].hist(\n",
    "        vae_recs[:, j].flatten().numpy(),\n",
    "        bins=bin_edges,\n",
    "        density=True,\n",
    "        alpha=0.5,\n",
    "        label=\"VAE Recon\",\n",
    "    )\n",
    "    axs[j].hist(\n",
    "        vae_rand[:, j].flatten().numpy(),\n",
    "        bins=bin_edges,\n",
    "        density=True,\n",
    "        alpha=0.5,\n",
    "        label=\"VAE Random\",\n",
    "    )\n",
    "    axs[j].hist(\n",
    "        lcfm_samp[:, j].flatten().numpy(),\n",
    "        bins=bin_edges,\n",
    "        density=True,\n",
    "        alpha=0.5,\n",
    "        label=\"LCFM\",\n",
    "    )\n",
    "    axs[j].set_yscale(\"log\")\n",
    "    axs[j].set_title(band)\n",
    "    if j == 0:\n",
    "        axs[j].legend(fontsize=7)\n",
    "\n",
    "fig.suptitle(\"Per-Band Pixel Distributions\", fontsize=14)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-summary-stats",
   "metadata": {},
   "source": [
    "### Summary Statistics <a id=\"summary-stats\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27-summary",
   "metadata": {},
   "outputs": [],
   "source": "# Summary statistics\nprint(\"=\" * 60)\nprint(\"TRAINING ANALYSIS SUMMARY\")\nprint(\"=\" * 60)\n\nprint(\"\\nVAE\")\nprint(f\"  Parameters: {n_vae_params:,}\")\nprint(f\"  Training steps: {vae_ckpt['global_step']}\")\nprint(f\"  Best loss: {vae_ckpt['best_loss']:.4e}\")\nif vae_loss_history:\n    last = vae_loss_history[-1]\n    print(f\"  Final train loss: {last['total_loss']:.4e}\")\n    if \"val_total_loss\" in last:\n        print(f\"  Final val loss: {last['val_total_loss']:.4e}\")\n\nprint(\"\\nLCFM\")\nprint(f\"  Total parameters: {n_lcfm_total:,}\")\nprint(f\"  Trainable parameters: {n_lcfm_trainable:,}\")\nprint(f\"  Training steps: {lcfm_ckpt['global_step']}\")\nprint(f\"  Best loss: {lcfm_ckpt['best_loss']:.4e}\")\nif lcfm_loss_history:\n    last = lcfm_loss_history[-1]\n    print(f\"  Final train loss: {last['total_loss']:.4e}\")\n    if \"val_total_loss\" in last:\n        print(f\"  Final val loss: {last['val_total_loss']:.4e}\")\n\nprint(\"\\nArtifact Paths\")\nprint(f\"  VAE ckpts: {output_dir / 'vae' / 'checkpoints'}\")\nprint(f\"  Encoder: {output_dir / 'encoder.pt'}\")\nprint(f\"  LCFM ckpts: {output_dir / 'lcfm' / 'checkpoints'}\")\nprint(f\"  LCFM samples: {output_dir / 'lcfm' / 'samples'}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}